# Plan: Ralph Loop Architecture (Claude First)

Date: 2026-02-12

## Confirmed Decisions

- Implementation language: Python.
- Default stop budget: `$25` per run.
- No separate quality-gate verifier stage.
- Experiment tracking should remain run-local (within `runs/`) unless proven insufficient.
- Preserve full run history, including tool-call/event logs, not just token/cost telemetry.
- Support resuming interrupted/stopped runs and extending limits.

## Proposed Architecture Shape

Use one generic architecture:

- `architectures/ralph_loop/arch.yaml`

Key hyperparameters:

- `agent.provider` (`claude` now, `codex` later)
- `agent.model` (e.g., Sonnet, Opus, Codex variant)
- `limits.max_cost_usd` (default `25.00`)
- `limits.max_time_minutes`
- optional `limits.max_tokens_total`
- optional `hyperparameters.max_iterations`

This keeps provider/model as experiment variables, not separate architecture families.

## Iteration and Stop Semantics

- Iteration definition: one outer-loop cycle that sends a "next research step" request, lets the agent complete internal turns/tool calls, and ends when a final result message is returned.
- Stops are enforced by the runner, not by a special model tool.
- Stop checks happen after each iteration and on startup/resume.
- Primary stop conditions:
- `max_cost_usd` (default `$25.00`)
- `max_time_minutes`
- optional cumulative tokens cap
- optional `max_iterations`
- SDK terminal stop subtypes (e.g., budget/turn-cap errors) are treated as stop events and logged.

## Logging and Traceability Requirements

- Record complete event history for future evaluation, including:
- prompts/responses metadata
- tool-call events and outputs metadata
- iteration boundaries and durations
- stop reasons and subtype
- cumulative usage and cost
- Persist run-local logs:
- `runs/<run_id>/logs/iterations.jsonl` for per-iteration summaries
- `runs/<run_id>/logs/events.jsonl` for fine-grained event/tool-call history
- `runs/<run_id>/state.json` for latest resumable state
- Keep logs append-only and timestamped for auditability.

## Pre-Compaction Checkpointing

- Use SDK `PreCompact` hook.
- The hook callback writes project-managed checkpoint files (not SDK-generated by default):
- `runs/<run_id>/research/checkpoints/precompact_<timestamp>.md`
- `runs/<run_id>/research/checkpoints/latest_precompact.md`
- Checkpoint content includes:
- current round and iteration
- recent findings
- open questions
- next planned actions
- unresolved errors/risks
- Purpose: preserve critical context on disk before compaction and improve resumability/recovery.

## Resume and Extension Behavior

- Resume from `runs/<run_id>/state.json` with stored session id and counters.
- Default resume mode continues the same session.
- Allow explicit limit extension on resume (for example cost/time increase), and log overrides.
- Provide fallback mode to start a new session from latest checkpoint if previous session cannot be resumed.
- On crash/shutdown, flush state and logs before exit where possible.

## Implementation Phases

1. Add generic `ralph_loop` architecture definition.
2. Add run manifest persistence so each run captures selected architecture, resolved hyperparameters, limits, provider/model, timestamps.
3. Implement Python runner outer loop with persistent session support and explicit stop-condition checks.
4. Add standardized loop memory files in run workspace: `research/findings.md`, `research/open_questions.md`, `research/strategies.md`, `research/current_round/*`, `results/strategies.csv`.
5. Add `PreCompact` hook callback and checkpoint file writing under `research/checkpoints/`.
6. Add full event and tool-call logging (`logs/events.jsonl`) and per-iteration summaries (`logs/iterations.jsonl`).
7. Add resumable state persistence (`state.json`) and resume/extend CLI behavior.
8. Run a low-budget smoke test and validate clean stop + resume behavior on configured limits.

## Prompt Protocol Direction (Agent Behavior)

The loop prompt should explicitly require the agent each iteration to:

1. Ask one concrete next research question.
2. Implement or update code needed to answer it.
3. Run analysis/backtest.
4. Save artifacts (figures/csv/report snippets).
5. Update research memory files.
6. Record strategy/experiment outcomes.
7. Propose the next iterationâ€™s hypothesis.

## Notes on Experiment Management

- Existing `runs/` structure should remain the primary experiment boundary.
- No backend DB is required at this stage.
- If needed, use CSV/JSONL within the run for per-iteration traceability.
- Main improvement needed now is explicit run-manifest persistence so hyperparameters are always recoverable from the run itself.

## Completion Criteria for This Feature

- `ralph_loop` architecture exists and is scaffoldable.
- Claude runner executes iterative autonomous rounds in a single session.
- Stops correctly when any configured limit is hit (including `$25` cost cap).
- Artifacts and memory files are written in a stable, reusable format.
- Full tool-call/event history and per-iteration telemetry are saved in run-local logs.
- Run metadata/hyperparameters and resumable state are persisted for reproducibility.
- Resume with limit extension works and is verified by smoke test.
